/home/ccwang/.local/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /20TB_06/dennislin0906/cvdl-hw4/train_ckpt exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]

  | Name    | Type     | Params | Mode
---------------------------------------------
0 | net     | PromptIR | 35.6 M | train
1 | loss_fn | L1Loss   | 0      | train
---------------------------------------------
35.6 M    Trainable params
0         Non-trainable params
35.6 M    Total params
142.369   Total estimated model params size (MB)
668       Modules in train mode
0         Modules in eval mode
Epoch 119: 100%|████████████████████████| 100/100 [00:33<00:00,  3.02it/s, v_num=6svd, train/loss_step=0.0214, val/loss=0.0227, val/psnr=29.20, train/loss_epoch=0.0252]                                                                                                        
                                                                                                                                                                                                                                                                                
/home/ccwang/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('train/loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
`Trainer.fit` stopped: `max_epochs=120` reached.
