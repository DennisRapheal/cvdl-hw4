/home/ccwang/.local/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /20TB_06/dennislin0906/cvdl-hw4/train_ckpt/model-5 exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]

  | Name    | Type     | Params | Mode
---------------------------------------------
0 | net     | PromptIR | 35.6 M | train
1 | loss_fn | L1Loss   | 0      | train
---------------------------------------------
35.6 M    Trainable params
0         Non-trainable params
35.6 M    Total params
142.369   Total estimated model params size (MB)
668       Modules in train mode
0         Modules in eval mode
Epoch 299: 100%|████████████████████████| 200/200 [00:49<00:00,  4.08it/s, v_num=oeb4, train/loss_step=0.0246, val/loss=0.0173, val/psnr=31.70, train/loss_epoch=0.0188]
                                                                                                                                                                        
/home/ccwang/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('train/loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
`Trainer.fit` stopped: `max_epochs=300` reached.
